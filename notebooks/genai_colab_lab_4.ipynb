{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFbQGw2POViM"
      },
      "source": [
        "# Lab 4 - RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSVyZRkvmqyc"
      },
      "source": [
        "## Setup Environment\n",
        "The following code loads the environment variables, images for the RAG App, and libraries required to run this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwWfBNdUmqyd"
      },
      "outputs": [],
      "source": [
        "FILE=\"GenAI Lab 4\"\n",
        "\n",
        "# ! pip install -qqq git+https://github.com/elastic/notebook-workshop-loader.git@main\n",
        "from notebookworkshoploader import loader\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "if os.path.isfile(\"../env\"):\n",
        "    load_dotenv(\"../env\", override=True)\n",
        "    print('Successfully loaded environment variables from local env file')\n",
        "else:\n",
        "    loader.load_remote_env(file=FILE, env_url=\"https://notebook-workshop-api-voldmqr2bq-uc.a.run.app\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ln-8SRvAI-jS"
      },
      "outputs": [],
      "source": [
        "# ! pip install -qqq tiktoken==0.5.2 cohere==4.38 openai==1.3.9\n",
        "# ! pip install -qqq streamlit==1.30.0 elasticsearch==8.12.0 elastic-apm==6.20.0 inquirer==3.2.1 python-dotenv==1.0.0\n",
        "# ! pip install -qqq elasticsearch-llm-cache==0.9.5\n",
        "! echo \"github codespaces has pre-installed these libraries\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0uQujqZclf0"
      },
      "source": [
        "## <font color=Green>Labs</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvZYvYkE62Df"
      },
      "source": [
        "### <font color=Orange>Lab 4.1 - Gathering Semantic documents from Elasticsearch</font>\n",
        "This first exercise will allow us to see an example of returing semantically matching documents from Elasticsearch.\n",
        "\n",
        "It is not too important to understand all the Elasticsearch DSL syntax at this stage.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsCwwEc95qv8"
      },
      "source": [
        "#### Run the code block below to set up the query function\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7lu2VBg6vMN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from elasticsearch import Elasticsearch\n",
        "import time\n",
        "import json\n",
        "import textwrap\n",
        "\n",
        "\n",
        "index = os.environ['ELASTIC_INDEX_DOCS_W']\n",
        "\n",
        "# Create Elasticsearch Connection\n",
        "es = Elasticsearch(\n",
        "            cloud_id=os.environ['ELASTIC_CLOUD_ID_W'],\n",
        "            api_key=(os.environ['ELASTIC_APIKEY_ID_W']),\n",
        "            request_timeout=30\n",
        "            )\n",
        "\n",
        "\n",
        "# Search Function\n",
        "def es_hybrid_search(question):\n",
        "    query = {\n",
        "      \"nested\": {\n",
        "        \"path\": \"passages\",\n",
        "        \"query\": {\n",
        "          \"bool\": {\n",
        "            \"must\": [\n",
        "              {\n",
        "                \"match\": {\n",
        "                  \"passages.text\": question\n",
        "                }\n",
        "              }\n",
        "            ]\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "\n",
        "    knn = {\n",
        "      \"inner_hits\": {\n",
        "        \"_source\": False,\n",
        "        \"fields\": [\n",
        "          \"passages.text\"\n",
        "        ]\n",
        "      },\n",
        "      \"field\": \"passages.embeddings\",\n",
        "      \"k\": 5,\n",
        "      \"num_candidates\": 100,\n",
        "      \"query_vector_builder\": {\n",
        "        \"text_embedding\": {\n",
        "          \"model_id\": \"sentence-transformers__all-distilroberta-v1\",\n",
        "          \"model_text\": question\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "\n",
        "    rank = {\n",
        "      \"rrf\": {}\n",
        "    }\n",
        "\n",
        "    fields = [\n",
        "      \"title\",\n",
        "      \"text\"\n",
        "    ]\n",
        "\n",
        "    size = 5\n",
        "\n",
        "    resp = es.search(index=index,\n",
        "                  #query=query,\n",
        "                  knn=knn,\n",
        "                  fields=fields,\n",
        "                  size=size,\n",
        "                  #rank=rank,\n",
        "                  source=False\n",
        "                  )\n",
        "\n",
        "    title_text = []\n",
        "    for doc in resp['hits']['hits']:\n",
        "      title_text.append( { 'title' : doc['fields']['title'][0],\n",
        "        'passage' : doc['inner_hits']['passages']['hits']['hits'][0]['fields']['passages'][0]['text'][0] }\n",
        "                         )\n",
        "\n",
        "    return title_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKBumt6W68wE"
      },
      "source": [
        "#### Example Semantic Search With Elastic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4hlknOP-Tba"
      },
      "outputs": [],
      "source": [
        "user_question = \"Who is Batman?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpHyxzev4WZm"
      },
      "outputs": [],
      "source": [
        "es_augment_docs = es_hybrid_search(user_question)\n",
        "\n",
        "print('Wikipedia titles returned:\\n')\n",
        "for hit, wiki in enumerate(es_augment_docs):\n",
        "  print(f\"{hit} - {wiki['title'] }\" )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPVcfU_26rGI"
      },
      "source": [
        "### <font color=Orange>Lab 4.2 - Sending Elasticsearch docs with a prompt for a RAG response</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZRE3N0q61L3"
      },
      "source": [
        "#### Run the code below to set up the LLM Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWeL5ANw65ND"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "import textwrap\n",
        "\n",
        "\n",
        "# Configure OpenAI client\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']\n",
        "openai.api_base = os.environ['OPENAI_API_BASE']\n",
        "openai.default_model = os.environ['OPENAI_API_ENGINE']\n",
        "openai.verify_ssl_certs = False\n",
        "client = OpenAI(api_key=openai.api_key, base_url=openai.api_base)\n",
        "\n",
        "if os.environ['ELASTIC_PROXY'] != \"True\":\n",
        "    openai.api_type = os.environ['OPENAI_API_TYPE']\n",
        "    openai.api_version = os.environ['OPENAI_API_VERSION']\n",
        "\n",
        "\n",
        "# Text wrapper for colab readibility\n",
        "def wrap_text(text):\n",
        "    wrapped_text = textwrap.wrap(text, 70)\n",
        "    return '\\n'.join(wrapped_text)\n",
        "\n",
        "\n",
        "# Function to connect with LLM\n",
        "def chat_gpt(client, question, passages):\n",
        "\n",
        "    system_prompt=\"You are a helpful assistant who answers questions from provided Wikipedia articles.\"\n",
        "    user_prompt = f'''Answer the followng question: {question}\n",
        "                    using only the wikipedia `passages` provided.\n",
        "                    If the answer is not provided in the `passages` respond ONLY with:\n",
        "                    \"I am unable to answer the user's question from the provided passage\" and nothing else.\n",
        "\n",
        "                  passages: {passages}\n",
        "\n",
        "                  AI response:\n",
        "                  '''\n",
        "\n",
        "    # Prepare the messages for the ChatGPT API\n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}]\n",
        "\n",
        "    response = client.chat.completions.create(model=openai.default_model,\n",
        "                                              temperature=0.2,\n",
        "                                              messages=messages,\n",
        "                                              )\n",
        "    return response\n",
        "#    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ4ZijSv65tQ"
      },
      "source": [
        "#### Pass the full prompt and wiki passages to LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MR-XrChD6-E0"
      },
      "outputs": [],
      "source": [
        "ai = chat_gpt(client, user_question, es_augment_docs)\n",
        "print(f\"User Question: \\n{user_question}\\n\")\n",
        "print(\"AI response:\")\n",
        "print(wrap_text(ai.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7RmurdZNPg-"
      },
      "source": [
        "### <font color=Orange>Lab 4.3 - Full RAG Application with UI</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Script\n",
        "We've placed the sample code in the streamlit folder of this repository\n",
        "\n",
        "Take a look at the code [streamlit/app.py](../streamlit/app.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu0KfS0ESf6e"
      },
      "source": [
        "## Streamlit\n",
        "To start the Streamlit app you need to use the ```streamlit run``` command from the folder.  You can do this either from this notebook or the Visual Studio Code terminal provided in Github Codespaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHIHFID3NBXa"
      },
      "outputs": [],
      "source": [
        "! cd ../streamlit; streamlit run app.py "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
