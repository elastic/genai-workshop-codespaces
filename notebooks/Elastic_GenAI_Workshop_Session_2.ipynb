{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFbQGw2POViM"
      },
      "source": [
        "# Lab 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Environment\n",
        "The following code loads the environment variables required to run this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "FILE=\"Elastic_GenAI_Workshop_session_2\"\n",
        "\n",
        "! pip install -qqq git+https://github.com/elastic/notebook-workshop-loader.git@main\n",
        "from notebookworkshoploader import loader\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "if os.path.isfile(\"../env\"):\n",
        "    load_dotenv(\"../env\", override=True)\n",
        "    print('Successfully loaded environment variables from local env file')\n",
        "else:\n",
        "    loader.load_remote_env(file=FILE, env_url=\"https://notebook-workshop-setup.elasticsa.co\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ln-8SRvAI-jS"
      },
      "outputs": [],
      "source": [
        "! pip install -q streamlit \"openai<1.0.0\" elasticsearch elastic-apm inquirer python-dotenv\n",
        "\n",
        "import os, inquirer, re, secrets, requests\n",
        "import streamlit as st\n",
        "import openai\n",
        "\n",
        "from IPython.display import display\n",
        "from ipywidgets import widgets\n",
        "from pprint import pprint\n",
        "from elasticsearch import Elasticsearch\n",
        "from string import Template\n",
        "from requests.auth import HTTPBasicAuth\n",
        "\n",
        "#if using the Elastic AI proxy, then generate the correct API key\n",
        "if os.environ['ELASTIC_PROXY'] == \"True\":\n",
        "\n",
        "  #remove the api type variable: it's a must when using the proxy\n",
        "  if \"OPENAI_API_TYPE\" in os.environ: del os.environ[\"OPENAI_API_TYPE\"]\n",
        "\n",
        "  #generate and share \"your\" unique hash\n",
        "  os.environ['USER_HASH'] = secrets.token_hex(nbytes=6)\n",
        "  print(f\"Your unique user hash is: {os.environ['USER_HASH']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEOV5KlycEmV"
      },
      "source": [
        "## Create Elasticsearch client connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUIB9n9ccGAR"
      },
      "outputs": [],
      "source": [
        "if 'ELASTIC_CLOUD_ID' in os.environ:\n",
        "  es = Elasticsearch(\n",
        "    cloud_id=os.environ['ELASTIC_CLOUD_ID'],\n",
        "    api_key=(os.environ['ELASTIC_APIKEY_ID'], os.environ['ELASTIC_APIKEY_SECRET']),\n",
        "    request_timeout=30\n",
        "  )\n",
        "elif 'ELASTIC_URL' in os.environ:\n",
        "  es = Elasticsearch(\n",
        "    os.environ['ELASTIC_URL'],\n",
        "    api_key=(os.environ['ELASTIC_APIKEY_ID'], os.environ['ELASTIC_APIKEY_SECRET']),\n",
        "    request_timeout=30\n",
        "  )\n",
        "else:\n",
        "  print(\"env needs to set either ELASTIC_CLOUD_ID or ELASTIC_URL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0uQujqZclf0"
      },
      "source": [
        "# Lab 2-1\n",
        "- Chunking (simplified example)\n",
        "- Generating embeddings\n",
        "- Perform kNN search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQQ-GAcOecAE"
      },
      "source": [
        "## Chunking\n",
        "Simplfied example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCArFD3kecTU"
      },
      "outputs": [],
      "source": [
        "body_content = \"Elastic Docs › Elasticsearch Guide [8.8] « Searchable snapshots Elasticsearch security principles » Secure the Elastic Stack edit The Elastic Stack is comprised of many moving parts. There are the Elasticsearch nodes that form the cluster, plus Logstash instances, Kibana instances, Beats agents, and clients all communicating with the cluster. To keep your cluster safe, adhere to the Elasticsearch security principles . The first principle is to run Elasticsearch with security enabled. Configuring security can be complicated, so we made it easy to start the Elastic Stack with security enabled and configured . For any new clusters, just start Elasticsearch to automatically enable password protection, secure internode communication with Transport Layer Security (TLS), and encrypt connections between Elasticsearch and Kibana. If you have an existing, unsecured cluster (or prefer to manage security on your own), you can manually enable and configure security to secure Elasticsearch clusters and any clients that communicate with your clusters. You can also implement additional security measures, such as role-based access control, IP filtering, and auditing. Enabling security protects Elasticsearch clusters by: Preventing unauthorized access with password protection, role-based access control, and IP filtering. Preserving the integrity of your data with SSL/TLS encryption. Maintaining an audit trail so you know who’s doing what to your cluster and the data it stores. If you plan to run Elasticsearch in a Federal Information Processing Standard (FIPS) 140-2 enabled JVM, see FIPS 140-2 . Preventing unauthorized access edit To prevent unauthorized access to your Elasticsearch cluster, you need a way to authenticate users in order to validate that a user is who they claim to be. For example, making sure that only the person named Kelsey Andorra can sign in as the user kandorra . The Elasticsearch security features provide a standalone authentication mechanism that enables you to quickly password-protect your cluster. If you’re already using LDAP, Active Directory, or PKI to manage users in your organization, the security features integrate with those systems to perform user authentication. In many cases, authenticating users isn’t enough. You also need a way to control what data users can access and what tasks they can perform. By enabling the Elasticsearch security features, you can authorize users by assigning access privileges to roles and assigning those roles to users. Using this role-based access control mechanism (RBAC), you can limit the user kandorra to only perform read operations on the events index restrict access to all other indices. The security features also enable you to restrict the nodes and clients that can connect to the cluster based on IP filters . You can block and allow specific IP addresses, subnets, or DNS domains to control network-level access to a cluster. See User authentication and User authorization . Preserving data integrity and confidentiality edit A critical part of security is keeping confidential data secured. Elasticsearch has built-in protections against accidental data loss and corruption. However, there’s nothing to stop deliberate tampering or data interception. The Elastic Stack security features use TLS to preserve the integrity of your data against tampering, while also providing confidentiality by encrypting communications to, from, and within the cluster. For even greater protection, you can increase the encryption strength . See Configure security for the Elastic Stack . Maintaining an audit trail edit Keeping a system secure takes vigilance. By using Elastic Stack security features to maintain an audit trail, you can easily see who is accessing your cluster and what they’re doing. You can configure the audit level, which accounts for the type of events that are logged. These events include failed authentication attempts, user access denied, node connection denied, and more. By analyzing access patterns and failed attempts to access your cluster, you can gain insights into attempted attacks and data breaches. Keeping an auditable log of the activity in your cluster can also help diagnose operational issues. See Enable audit logging . « Searchable snapshots Elasticsearch security principles » Most Popular Video Get Started with Elasticsearch Video Intro to Kibana Video ELK for Logs & Metrics\"\n",
        "print (\"The length of the paragraph is %s characters\" % len (body_content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NGORlTWncWt"
      },
      "source": [
        "There are many ways to split text. We can split on individual characters, spaces, at a set length, using a library like langchain, or using a tokenizer, to name a few ways.\n",
        "\n",
        "For this simple example we are going to split on dot+space \". \", essentially spliting individual sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVu0PcLzolSX"
      },
      "outputs": [],
      "source": [
        "chunked_content = [chunk for chunk in re.split('\\. ',  body_content)]\n",
        "chunk = chunked_content[0] # We'll use this later\n",
        "print (\"There are now %s sentence chunks.\\nThe first element is:'%s'\" % (len(chunked_content), chunk))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7NHs6s2qYii"
      },
      "source": [
        "TODO Talk about tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwAOqksbrOUv"
      },
      "outputs": [],
      "source": [
        "# Show the \"tokens\" from the first chunk\n",
        "chunk.split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DL2MkCprTBe"
      },
      "source": [
        "## Generate embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3xI7Rg8rYOL"
      },
      "source": [
        "We need to pass our text to an embedding model to generate vectors.\n",
        "\n",
        "Models have pre-definied token limits which restrict the amount of text (tokens really) that can be processed into vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Co_jW0DevAUY"
      },
      "outputs": [],
      "source": [
        "es_model_id = 'sentence-transformers__msmarco-minilm-l-12-v3'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXVW_fOpsr0H"
      },
      "outputs": [],
      "source": [
        "chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8MOh-2Fqbr1"
      },
      "outputs": [],
      "source": [
        "docs =  [\n",
        "    {\n",
        "      \"text_field\": chunk\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-n1lTvAvVht"
      },
      "outputs": [],
      "source": [
        "chunk_vector = es.ml.infer_trained_model(model_id=es_model_id, docs=docs, )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XHbhuX4v3ky"
      },
      "outputs": [],
      "source": [
        "vector_doc = {\n",
        "  \"_index\": \"chunker\",\n",
        "  \"_id\": \"64837860d86b1293a9a5f620-0\",\n",
        "  \"_source\": {\n",
        "      \"chunk\" : chunk,\n",
        "      \"chunk-vector\" : chunk_vector['inference_results'][0]['predicted_value'],\n",
        "      \"body_content\" : body_content\n",
        "  }\n",
        "}\n",
        "\n",
        "pprint(vector_doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEK9CFy9w-6d"
      },
      "source": [
        "## Exceeding the model's token limit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ndjrwim0XS-m"
      },
      "source": [
        "Let's take a look at what happens when we exceed the model's token limit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVIaFOG-yaKJ"
      },
      "outputs": [],
      "source": [
        "full_paragraph =  [\n",
        "    {\n",
        "      \"text_field\": body_content\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6msjDqmZyaKJ"
      },
      "outputs": [],
      "source": [
        "chunk_vector = es.ml.infer_trained_model(model_id=es_model_id, docs=full_paragraph, )\n",
        "print(\"When the token size exceeds the model's max token limit, the value of `is_truncated` will return True\")\n",
        "print('We exceeded the model token limit: %s' % chunk_vector['inference_results'][0]['is_truncated'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYMM629tXYF8"
      },
      "source": [
        "We see that the model still processed the tokens up to it's limit, then simply truncated (ignored) any tokens longer than that.\n",
        "\n",
        "Elasticsearch returns a `is_truncated : True` key:value to let you know the embedding returned is not for the full text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIbhhCa4oKPO"
      },
      "source": [
        "## Querying with hybrid vector search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFNyCWCWp_nI"
      },
      "source": [
        "We will run through an example of searching with approximate kNN vector search combined with BM25 text search combing the results with rrf.\n",
        "\n",
        "This is the type of query that will power the UI we will use in lab 2-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVqwwfskoKqP"
      },
      "outputs": [],
      "source": [
        "def search_with_knn(query_text, es):\n",
        "    # Elasticsearch query (BM25) and kNN configuration for rrf hybrid search\n",
        "\n",
        "    query = {\n",
        "        \"bool\": {\n",
        "            \"must\": [{\n",
        "                \"match\": {\n",
        "                    \"body_content\": {\n",
        "                        \"query\": query_text\n",
        "                    }\n",
        "                }\n",
        "            }],\n",
        "            \"filter\": [{\n",
        "              \"term\": {\n",
        "                \"url_path_dir3\": \"elasticsearch\"\n",
        "              }\n",
        "            }]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    knn = [\n",
        "    {\n",
        "      \"field\": \"chunk-vector\",\n",
        "      \"k\": 10,\n",
        "      \"num_candidates\": 10,\n",
        "      \"filter\": {\n",
        "        \"bool\": {\n",
        "          \"filter\": [\n",
        "            {\n",
        "              \"range\": {\n",
        "                \"chunklength\": {\n",
        "                  \"gte\": 0\n",
        "                }\n",
        "              }\n",
        "            },\n",
        "            {\n",
        "              \"term\": {\n",
        "                \"url_path_dir3\": \"elasticsearch\"\n",
        "              }\n",
        "            }\n",
        "          ]\n",
        "        }\n",
        "      },\n",
        "      \"query_vector_builder\": {\n",
        "        \"text_embedding\": {\n",
        "          \"model_id\": \"sentence-transformers__msmarco-minilm-l-12-v3\",\n",
        "          \"model_text\": query_text\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "\n",
        "    rank = {\n",
        "       \"rrf\": {\n",
        "       }\n",
        "   }\n",
        "\n",
        "    fields= [\n",
        "        \"title\",\n",
        "        \"url\",\n",
        "        \"body_content\"\n",
        "      ]\n",
        "\n",
        "    resp = es.search(index=os.environ['ELASTIC_INDEX_DOCS'],\n",
        "                     query=query,\n",
        "                     knn=knn,\n",
        "                     rank=rank,\n",
        "                     fields=fields,\n",
        "                     size=1,\n",
        "                     source=False)\n",
        "\n",
        "    return resp\n",
        "\n",
        "query = 'How do I start Elastic with Security Enabled?'\n",
        "response = search_with_knn(query, es)\n",
        "pprint(response['hits'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCsxLt8Yc2LI"
      },
      "source": [
        "# Lab 2-2\n",
        "RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELRm5s9pc5Jg"
      },
      "source": [
        "## Verify our Elasticsearch connection is still active\n",
        "If you receive an error, rerun the cells in the Setup section above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Isssrm07dKMe"
      },
      "outputs": [],
      "source": [
        "print(es.info()['tagline']) # should return cluster info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7RmurdZNPg-"
      },
      "source": [
        "## Main Script\n",
        "We've placed the sample code in the streamlit folder of this repository\n",
        "\n",
        "Take a look at the code [streamlit/app.py](../streamlit/app.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu0KfS0ESf6e"
      },
      "source": [
        "## Streamlit\n",
        "To start the Streamlit app you need to use the ```streamlit run``` command from the folder.  You can do this either from this notebook or the Visual Studio Code terminal provided in Github Codespaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHIHFID3NBXa"
      },
      "outputs": [],
      "source": [
        "! cd ../streamlit; streamlit run app.py "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
